# Copyright Â© 2023 Province of British Columbia
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""The BOR solr data import service."""
import gc
import sys

from flask import current_app
from search_api.exceptions import SolrException

from search_solr_importer import create_app
from search_solr_importer.utils import (collect_btr_data, collect_colin_data, collect_lear_data,
                                        prep_data, prep_data_btr, reindex_post, reindex_prep,
                                        reindex_recovery, resync, update_solr)


def load_search_core():  # pylint: disable=too-many-statements,too-many-locals,too-many-branches; will update
    """Load data from LEAR and COLIN into the search core."""
    try:
        is_reindex = current_app.config.get('REINDEX_CORE')
        is_preload = current_app.config.get('PRELOADER_JOB')
        include_btr_load = current_app.config.get('INCLUDE_BTR_LOAD')
        include_colin_load = current_app.config.get('INCLUDE_COLIN_LOAD')
        include_lear_load = current_app.config.get('INCLUDE_LEAR_LOAD')
        final_record = None

        if is_reindex and current_app.config.get('IS_PARTIAL_IMPORT'):
            current_app.logger.error('Attempted reindex on partial data set.')
            current_app.logger.debug('Setting reindex to False to prevent potential data loss.')
            is_reindex = False

        if is_reindex:
            current_app.logger.debug('---------- Pre Reindex Actions ----------')
            reindex_prep(is_preload)

        try:
            colin_count = 0
            if include_colin_load:
                current_app.logger.debug('---------- Collecting/Importing COLIN Entities ----------')
                colin_data_cur = collect_colin_data()
                current_app.logger.debug('Fetching corp batch rows...')
                colin_data = colin_data_cur.fetchall()
                colin_data_descs = [desc[0].lower() for desc in colin_data_cur.description]
                colin_data_cur.close()
                # NB: need full data set under each corp num to collapse parties properly
                current_app.logger.debug('********** Mapping COLIN Entities **********')
                prepped_colin_data = prep_data(colin_data,
                                               colin_data_descs,
                                               'COLIN')
                current_app.logger.debug(f'COLIN businesses ready for import: {len(prepped_colin_data)}')
                # execute update to solr in batches
                current_app.logger.debug('********** Importing COLIN Entities **********')
                colin_count = update_solr(prepped_colin_data, 'COLIN')
                # free up memory
                final_record = [prepped_colin_data[-1]], 'COLIN', False
                del colin_data, prepped_colin_data
                gc.collect()
                current_app.logger.debug(f'COLIN import completed. Total COLIN businesses imported: {colin_count}.')

            lear_count = 0
            if include_lear_load:
                current_app.logger.debug('---------- Collecting LEAR Entities ----------')
                lear_data_cur = collect_lear_data()
                lear_data = lear_data_cur.fetchall()

                current_app.logger.debug('---------- Mapping LEAR data ----------')
                prepped_lear_data = prep_data(
                    data=lear_data,
                    data_descs=[desc[0].lower() for desc in lear_data_cur.description],
                    source='LEAR',
                )
                current_app.logger.debug(f'{len(prepped_lear_data)} LEAR records ready for import.')

                # execute update to solr in batches
                current_app.logger.debug('---------- Importing LEAR entities ----------')
                lear_count = update_solr(prepped_lear_data, 'LEAR')
                # free up memory
                final_record = [prepped_lear_data[-1]], 'LEAR', False
                del lear_data, prepped_lear_data
                gc.collect()
                current_app.logger.debug(f'LEAR import completed. Total LEAR businesses imported: {lear_count}')

            current_app.logger.debug(f'Total businesses imported: {colin_count + lear_count}')

            total_btr_count = 0
            if include_btr_load:
                current_app.logger.debug('---------- Collecting/Importing BTR Data ----------')
                btr_fetch_count = 0
                batch_limit = current_app.config.get('BTR_BATCH_LIMIT')
                btr_data_descs = []
                loop_count = 0

                while loop_count < 100:  # NOTE: should never get to this condition
                    loop_count += 1
                    current_app.logger.debug('********** Collecting BTR data **********')
                    btr_data_cur = collect_btr_data(batch_limit, btr_fetch_count)
                    btr_data = btr_data_cur.fetchall()
                    btr_fetch_count += len(btr_data)
                    btr_data_cur.close()
                    if not btr_data:
                        break

                    current_app.logger.debug('********** Mapping BTR data **********')
                    if not btr_data_descs:
                        # just need to do once
                        btr_data_descs = [desc[0].lower() for desc in btr_data_cur.description]
                    prepped_btr_data = prep_data_btr(btr_data, btr_data_descs)
                    current_app.logger.debug(f'{len(prepped_btr_data)} BTR records ready for import.')

                    current_app.logger.debug('********** Importing BTR entities **********')
                    total_btr_count += update_solr(prepped_btr_data, 'BTR', True)
                    current_app.logger.debug(f'BTR batch import completed. Records imported: {total_btr_count}.')
                    # free up memory
                    final_record = [prepped_btr_data[-1]], 'BTR', True
                    del btr_data, prepped_btr_data
                    gc.collect()

                current_app.logger.debug(f'BTR import completed. Total BTR partial records imported: {total_btr_count}')

        except Exception as err:  # noqa: B902
            if is_reindex and not is_preload:
                reindex_recovery()
            raise err  # pass along

        try:
            current_app.logger.debug('---------- Resync ----------')
            resync()
        except Exception as error:  # noqa: B902
            current_app.logger.debug(error.with_traceback(None))
            current_app.logger.error('Resync failed.')

        try:
            current_app.logger.debug('---------- Final Commit ----------')
            current_app.logger.debug('Triggering final commit on leader to make changes visible to search...')
            update_solr(final_record[0], final_record[1], final_record[2])
            current_app.logger.debug('Final commit complete.')

        except Exception as error:  # noqa: B902
            current_app.logger.debug(error.with_traceback(None))
            current_app.logger.error('Final commit failed. (This will only effect DEV).')

        if is_reindex and not is_preload:
            current_app.logger.debug('---------- Post Reindex Actions ----------')
            reindex_post()

        current_app.logger.debug('SOLR import finished successfully.')

    except SolrException as err:
        current_app.logger.debug(f'SOLR gave status code: {err.status_code}')
        current_app.logger.error(err.error)
        current_app.logger.debug('SOLR import failed.')
        sys.exit(1)


if __name__ == '__main__':
    print('Starting data importer...')
    app = create_app()  # pylint: disable=invalid-name
    with app.app_context():
        load_search_core()
        sys.exit(0)
